{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.6937e+01 -2.2388e+01  6e+02  3e+01  3e-14\n",
      " 1: -6.1861e+00 -2.1307e+01  2e+01  4e-01  5e-14\n",
      " 2: -6.0846e+00 -7.9512e+00  2e+00  4e-03  8e-15\n",
      " 3: -6.6184e+00 -7.0236e+00  4e-01  5e-04  8e-15\n",
      " 4: -6.7782e+00 -6.8419e+00  6e-02  6e-05  9e-15\n",
      " 5: -6.8062e+00 -6.8211e+00  1e-02  1e-05  1e-14\n",
      " 6: -6.8143e+00 -6.8157e+00  1e-03  4e-07  1e-14\n",
      " 7: -6.8148e+00 -6.8151e+00  3e-04  9e-08  9e-15\n",
      " 8: -6.8149e+00 -6.8150e+00  1e-04  1e-08  9e-15\n",
      " 9: -6.8150e+00 -6.8150e+00  1e-06  1e-10  9e-15\n",
      "Optimal solution found.\n",
      "72 support vectors out of 100 points\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.9669e+01 -1.8685e+01  5e+02  2e+01  3e-14\n",
      " 1: -3.7174e+00 -1.7276e+01  3e+01  7e-01  4e-14\n",
      " 2: -3.0274e+00 -8.7555e+00  9e+00  2e-01  1e-14\n",
      " 3: -2.4772e+00 -3.8517e+00  2e+00  3e-02  7e-15\n",
      " 4: -2.6786e+00 -3.0021e+00  4e-01  6e-03  6e-15\n",
      " 5: -2.7480e+00 -2.8489e+00  1e-01  2e-03  6e-15\n",
      " 6: -2.7739e+00 -2.8023e+00  3e-02  3e-04  9e-15\n",
      " 7: -2.7841e+00 -2.7864e+00  3e-03  2e-05  8e-15\n",
      " 8: -2.7850e+00 -2.7851e+00  3e-05  3e-07  1e-14\n",
      " 9: -2.7850e+00 -2.7850e+00  3e-07  3e-09  9e-15\n",
      "Optimal solution found.\n",
      "39 support vectors out of 100 points\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5414e+00 -1.4118e+01  4e+02  2e+01  3e-14\n",
      " 1: -1.0842e+00 -1.2598e+01  3e+01  9e-01  2e-14\n",
      " 2: -5.1952e-01 -4.8301e+00  6e+00  1e-01  4e-15\n",
      " 3: -3.5745e-01 -9.5008e-01  7e-01  1e-02  3e-15\n",
      " 4: -4.5246e-01 -6.3776e-01  2e-01  2e-03  3e-15\n",
      " 5: -4.8759e-01 -5.6282e-01  8e-02  6e-04  4e-15\n",
      " 6: -5.1088e-01 -5.3373e-01  2e-02  2e-16  5e-15\n",
      " 7: -5.1986e-01 -5.2191e-01  2e-03  2e-16  5e-15\n",
      " 8: -5.2070e-01 -5.2081e-01  1e-04  2e-16  5e-15\n",
      " 9: -5.2074e-01 -5.2075e-01  3e-06  2e-16  6e-15\n",
      "10: -5.2074e-01 -5.2074e-01  7e-08  2e-16  5e-15\n",
      "Optimal solution found.\n",
      "10 support vectors out of 100 points\n",
      "[0 2 1 0 0 2 0 1 0 0 1 2 2 2 2 1 1 0 0 1 2 1 2 1 1 1 1 1 2 2 2 2 0 2 2 1 0\n",
      " 2 2 2 1 0 0 2 2 0 1 1 0 1]\n",
      "[0 2 1 0 0 2 0 1 0 0 1 2 2 2 2 0 1 0 0 1 2 1 2 1 1 1 1 1 2 2 2 2 0 2 2 1 0\n",
      " 2 2 2 1 0 0 2 2 0 1 1 0 1]\n",
      "49 out of 50 predictions correct\n"
     ]
    }
   ],
   "source": [
    "''' This is a multi-class and multi-feature SVM implementation using linear kernel.\n",
    "    It is tested using iris flower data set.\n",
    "    The four features are sepal length, sepal width, petal length and petal width.\n",
    "    The three classes are Versicolour (class 0), Virginica (class 1) and Setosa (class 2).\n",
    "    There are 100 training samples and 50 testing samples.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, min_lagmult=1e-6, C=0.1):\n",
    "        self.C = C                      #C parameter\n",
    "        self.min_lagmult = min_lagmult    #support vector's min Lagrange multiplier value\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = np.dot(X[i], X[j])\n",
    "        \"\"\"\n",
    "        To build QP problem and solve it using cvxopt.solver.qp,\n",
    "        optimizing this form:\n",
    "        \n",
    "        min (1/2)*x^T*P*x + q^T*x\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            lower_bound = np.diag(np.ones(n_samples) * -1)\n",
    "            upper_bound = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((lower_bound, upper_bound)))\n",
    "            lower_bound = np.zeros(n_samples)\n",
    "            upper_bound = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((lower_bound, upper_bound)))\n",
    "\n",
    "        # solve QP problem\n",
    "        result = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(result['x'])\n",
    "        \n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > self.min_lagmult\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print (\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b = self.b / len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        self.w = np.zeros(n_features)\n",
    "        for n in range(len(self.a)):\n",
    "            self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * np.dot(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.project(X)\n",
    "\n",
    "#fletch the data and pregrocceing\n",
    "\n",
    "iris_x_train=pd.read_csv('C:/jupyternotebook/iris_X_train.csv')\n",
    "iris_y_train=pd.read_csv('C:/jupyternotebook/iris_y_train.csv')\n",
    "iris_x_test=pd.read_csv('C:/jupyternotebook/iris_X_test.csv')\n",
    "iris_y_test=pd.read_csv('C:/jupyternotebook/iris_y_test.csv')\n",
    "\n",
    "iris_x_test = np.array(iris_x_test)\n",
    "iris_x_train = np.array(iris_x_train)\n",
    "iris_y_train = np.squeeze(np.array(iris_y_train))\n",
    "iris_y_train1 = iris_y_train\n",
    "iris_y_train2 = iris_y_train\n",
    "iris_y_train = iris_y_train + 0.0\n",
    "iris_y_train1 = iris_y_train1 + 0.0\n",
    "iris_y_train2 = iris_y_train2 + 0.0\n",
    "iris_y_test = np.squeeze(np.array(iris_y_test))\n",
    "\n",
    "for i, k in enumerate(iris_y_train):\n",
    "    if iris_y_train[i]!=0.0: iris_y_train[i] = -1.0\n",
    "    else: iris_y_train[i] = 1.0\n",
    "\n",
    "\n",
    "for i, k in enumerate(iris_y_train1):\n",
    "    if iris_y_train1[i]!=1.0: iris_y_train1[i] = -1.0\n",
    "    else: iris_y_train1[i] = 1.0\n",
    "\n",
    "\n",
    "for i, k in enumerate(iris_y_train2):\n",
    "    if iris_y_train2[i]!=2.0 : iris_y_train2[i] = -1.0\n",
    "    else: iris_y_train2[i] = 1.0\n",
    "\n",
    "\n",
    "#train the svm using ovo method\n",
    "clf = SVM()\n",
    "\n",
    "clf.fit(iris_x_train, iris_y_train)\n",
    "y_predict = clf.predict(iris_x_test)\n",
    "class0 = y_predict\n",
    "\n",
    "clf.fit(iris_x_train, iris_y_train1)\n",
    "y_predict = clf.predict(iris_x_test)\n",
    "class1 = y_predict\n",
    "\n",
    "clf.fit(iris_x_train, iris_y_train2)\n",
    "y_predict = clf.predict(iris_x_test)\n",
    "class2 = y_predict\n",
    "\n",
    "np.column_stack((class0,class1,class2))\n",
    "b = np.column_stack((class0,class1,class2))\n",
    "\n",
    "result = np.zeros(len(iris_y_test))\n",
    "for i, k in enumerate(b):\n",
    "    result[i] = np.argmax(b[i])\n",
    "result = result.astype(int)\n",
    "\n",
    "print(result)\n",
    "print(iris_y_test)\n",
    "\n",
    "correct = np.sum(result == iris_y_test)\n",
    "print (\"%d out of %d predictions correct\" % (correct, len(iris_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
